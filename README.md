<h1 align="center">
  <br>
  <a><img src="assets/out of 311-logos.png" alt="logo" width=200px></a>
</h1>

<h4 align="center">KU Study Group</h4>
<p align="center">NLP & AI, Korea University</p>

<p align="center">
    <img alt="python-3.7.7" src="https://img.shields.io/badge/NLP--blue"/>
    <img alt="django-2.2.5" src="https://img.shields.io/badge/Machine Learning--yellow"/>
    <img alt="chromedriver-79.0.3945" src="https://img.shields.io/badge/Math in AI--blueviolet"/>
    <img alt="GitHub" src="https://img.shields.io/github/license/metterian/redbttn-seoul-studio"/>
</p>


## Purpose

## For What

### Background

- Auto Regressive
- Auto Encoding

### Before Pre-trained

- Word2Vec
  - [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
- Seq2Seq
  - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- RNN
  - [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network](https://arxiv.org/abs/1808.03314)

### After Pre-trained

- Attention Machanism
  - 고정된 벡터에 정보 압축시 정보 손실 발생
  - RNN: Vanishing Gradient
  - [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf))
- Seq2Seq + Attention
- Transformer
  - [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- GPT-1
  - **[OpenAI GPT-1 - Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)**
- BERT
  - [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- XLNet
  - [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
- RoBERTa
  - [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
- ALBERT
  - [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
- T5
  - [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)
- ELECTRA
  - [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555)
- GPT3
  - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)



## Book
![BERT BOOK](https://books.google.co.kr/books/content?id=50zyzQEACAAJ&printsec=frontcover&img=1&zoom=1&imgtk=AFLRE71HM_XTVTIWBksRKgpSp-Ju0W5H48t05VKNNmc2L9QYKOb5gt6zWVnnsJZZ-cDaAaooJrRpbgTAi8u6JV2b_-XFCH486yeCMzbGYc75c8XvH_f86_xaOvtt3fRqvMOQSzSM4Xtj)

- [link](https://books.google.co.kr/books/about/Getting_Started_with_Google_BERT.html?id=50zyzQEACAAJ&source=kp_author_description&redir_esc=y)

## Videos and Online Courses



## Tools


## Meet Rules

- Scrum day: the day of start of week
  Explain what you learned and what you will learn
- Main Study: explain what you learned
- Memorize: write in markdown that you can not explain or hesitate to explain


## Role of part


